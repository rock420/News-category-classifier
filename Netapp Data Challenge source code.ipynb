{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for prepocessing,featuring,splitting dataset,training \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import svm\n",
    "from gensim.models.doc2vec import Doc2Vec,TaggedDocument\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV,learning_curve\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score,confusion_matrix,recall_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(\"dataset.json\")   #read datasheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for prepocessing of data\n",
    "def prepocess(array):\n",
    "    array=array.str.lower()            #lower all alphabets \n",
    "    array=array.str.replace('\\d+','')   #removing digits\n",
    "    punc_table = str.maketrans({key: None for key in string.punctuation})\n",
    "    array=array.str.translate(punc_table)            #removing puntuations\n",
    "    from nltk.corpus import stopwords \n",
    "    stop=set(stopwords.words('english'))            #removing stopwards\n",
    "    snow=nltk.stem.SnowballStemmer('english')        #stemming of words\n",
    "    array=array.apply(lambda sentance: ' '.join([snow.stem(word) for word in sentance.split() if word not in stop]))\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for undersampling the data \n",
    "indices={category:np.array(df[df['category']==category].index) for category in df['category'].unique() }\n",
    "def undersample(indices):\n",
    "    for category in indices.keys():\n",
    "        if (df['category'].value_counts()[category]>3000):\n",
    "            indices[category]=np.array(np.random.choice(indices[category],np.random.randint(1004,3000),replace=False))\n",
    "    undersample=np.concatenate(list(indices.values()))\n",
    "    undersample_data=df.iloc[undersample,:]\n",
    "    return (undersample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=undersample(indices)\n",
    "data.drop('id',axis=1)\n",
    "y=list(data['category'].unique())\n",
    "no=[i for i in range(0,41)]\n",
    "dic={catg:i for (catg,i) in zip(y,no) }\n",
    "Y=data['category'].apply(lambda x:dic[x] )\n",
    "X_data=data['headline']+' '+data['short_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=prepocess(X_data)\n",
    "X=np.array(X)\n",
    "Y=np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train and test \n",
    "split = StratifiedShuffleSplit(n_splits=5, test_size=0.40)\n",
    "for train_index, test_index in split.split(X, Y):\n",
    "    x_train, x_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to fit the doc2vec model using training data and to transform the training data to give feature matrix\n",
    "class Doc2VecModel():\n",
    "\n",
    "    def __init__(self, dm=1, vector_size=200, window=1,min_count=1,workers=4):\n",
    "        self.d2v_model = None\n",
    "        self.vector_size = vector_size\n",
    "        self.window = window\n",
    "        self.dm = dm\n",
    "        self.min_count=min_count\n",
    "        self.workers=workers\n",
    "\n",
    "    def fit(self, x_train, y=None):\n",
    "        # Initialize model\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, window=self.window, dm=self.dm,min_count=5,workers=self.workers)\n",
    "        tagged_documents=[TaggedDocument(row.split(), [i])  for row,i in zip(x_train,range(len(x_train)))]\n",
    "        # Build vocabulary\n",
    "        self.d2v_model.build_vocab(tagged_documents)\n",
    "        # Train model\n",
    "        self.d2v_model.train(tagged_documents, total_examples=len(tagged_documents), epochs=10)\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_train):\n",
    "        train_set=np.array([self.d2v_model.infer_vector(row.split(),steps=10) for row in x_train])\n",
    "        return train_set\n",
    "\n",
    "    def fit_transform(self, x_train, y=None):\n",
    "        self.fit(x_train)\n",
    "        return self.transform(x_train)\n",
    "    def set_params(self,**params):\n",
    "        for param, value in params.items():\n",
    "            setattr(self, param, value)\n",
    "        return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting learning curve to visualize whether the model has high bias or high variance\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "  \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "title = \"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = svm.SVC(gamma=0.8,C=1)                     #support vector machine classifier\n",
    "plot_learning_curve(estimator, title, feature_train,label_train , (0.7, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter tuning using gridsearchcv\n",
    "param_grid = {'doc2vec__window': [1,2, 3],\n",
    "              'doc2vec__dm': [0,1],\n",
    "              'doc2vec__size': [i for i in range(400,1000,30)],\n",
    "              'clf__C': [0.1*i for i in range(8,50,3)]\n",
    "}\n",
    "\n",
    "pipe_log = Pipeline([('doc2vec', Doc2VecModel()), ('clf', svm.SVC(gamma=1))])\n",
    "\n",
    "svm_clf = GridSearchCV(pipe_log, \n",
    "                        param_grid=param_grid,\n",
    "                        scoring=\"accuracy\",\n",
    "                        verbose=3,\n",
    "                        n_jobs=-1,cv=5)\n",
    "svm_clf.fit(x_train,y_train)\n",
    "svm_clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('doc2vec', <__main__.Doc2VecModel object at 0x000002954B53CCF8>), ('clf', SVC(C=2, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.7, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline to run doc2vec and classifier \n",
    "pipe_log = Pipeline([('doc2vec', Doc2VecModel(min_count=7,vector_size=1000,workers=4,dm=1)), ('clf', svm.SVC(C=2,gamma=0.7,kernel='rbf'))])\n",
    "pipe_log.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted=pipe_log.predict(x_test)\n",
    "label_test=np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix=confusion_matrix(label_test,predicted)\n",
    "fig= plt.figure(figsize=(18,8))\n",
    "sns.heatmap(cnf_matrix,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n",
    "plt.title(\"Confusion_matrix\")\n",
    "plt.xlabel(\"Predicted_class\")\n",
    "plt.ylabel(\"Real class\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n----------F1_score------------------------------------\")\n",
    "print(f1_score(label_test,predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------submission file---------------------------------------------------------------#\n",
    "\n",
    "\n",
    "test=pd.read_json(\"datasetresult.json\")\n",
    "test_data=test['headline']+' '+test['short_description']\n",
    "test_data=prepocess(test_data)\n",
    "test_data=np.array(test_data)\n",
    "predicted_data=pipe_log.predict(test_data)\n",
    "catg_list=list(dic.keys())\n",
    "val_list=list(dic.values())\n",
    "prediction=[catg_list[val_list.index(data)] for data in predicted_data]\n",
    "submission_data=pd.DataFrame({'id':test['id'],'category':prediction})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_data.to_json(\"submission.json\",orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
